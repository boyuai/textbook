{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用CycleGAN进行风格迁移\n",
    "训练部分代码参考了https://github.com/tjwei/GANotebooks/blob/master/CycleGAN-keras.ipynb\n",
    "\n",
    "### 功能简述\n",
    "\n",
    "可以实现A类图片和B类图片之间相互的风格迁移。\n",
    "\n",
    "比如A类图片为马的图片，B类图片为斑马的图片，可以实现将马转化成斑马，也可以将斑马转化成马。\n",
    "\n",
    "整体架构使用的是CycleGAN，即同时训练将A转化为B风格的GAN和将B转化为A风格的GAN。\n",
    "\n",
    "训练耗费的时间比较长，但是一旦有了训练好的模型，生成图片的速度比较快。\n",
    "\n",
    "### 目录结构：\n",
    "\n",
    "    /--+-- snapshot/                  ...存放快照\n",
    "       |\n",
    "       +-- models/                    ...存放训练出来的模型\n",
    "       |\n",
    "       +-- data/                      ...存放数据\n",
    "             |\n",
    "             +-- vangogh2photo/       ...某个数据集\n",
    "                       |\n",
    "                       +-- trainA/    ...类别为A的图片\n",
    "                       |\n",
    "                       +-- trainB/    ...类别为B的图片\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集相关\n",
    "\n",
    "在我们的实现中，使用了python中的“类”的概念，可以把它看作一种工具。一种类就是完成一种任务的工具，包含了完成这种任务的一些数据和处理这些数据的方法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_image(fn, image_size):\n",
    "    \"\"\"\n",
    "    加载一张图片\n",
    "    fn:图像文件路径\n",
    "    image_size:图像大小\n",
    "    \"\"\"\n",
    "    im = Image.open(fn).convert('RGB')\n",
    "    \n",
    "    #切割图像(截取图像中间的最大正方形，然后将大小调整至输入大小)\n",
    "    if (im.size[0] >= im.size[1]):\n",
    "        im = im.crop(((im.size[0] - im.size[1])//2, 0, (im.size[0] + im.size[1])//2, im.size[1]))\n",
    "    else:\n",
    "        im = im.crop((0, (im.size[1] - im.size[0])//2, im.size[0], (im.size[0] + im.size[1])//2))\n",
    "    im = im.resize((image_size, image_size), Image.BILINEAR)\n",
    "    \n",
    "    #将0-255的RGB值转换到[-1,1]上的值\n",
    "    arr = np.array(im)/255*2-1   \n",
    "    \n",
    "    return arr\n",
    "\n",
    "import glob\n",
    "import random\n",
    "\n",
    "class DataSet(object):\n",
    "    \"\"\"\n",
    "    用于管理数据的类\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, image_size = 256):\n",
    "        self.data_path = data_path\n",
    "        self.epoch = 0\n",
    "        self.__init_list()\n",
    "        self.image_size = image_size\n",
    "        \n",
    "    def __init_list(self):\n",
    "        self.data_list = glob.glob(self.data_path)\n",
    "        random.shuffle(self.data_list)\n",
    "        self.ptr = 0\n",
    "        \n",
    "    def get_batch(self, batchsize):\n",
    "        \"\"\"\n",
    "        取出batchsize张图片\n",
    "        \"\"\"\n",
    "        if (self.ptr + batchsize >= len(self.data_list)):\n",
    "            batch = [load_image(x, self.image_size) for x in self.data_list[self.ptr:]]\n",
    "            rest = self.ptr + batchsize - len(self.data_list)\n",
    "            self.__init_list()\n",
    "            batch.extend([load_image(x, self.image_size) for x in self.data_list[:rest]])\n",
    "            self.ptr = rest\n",
    "            self.epoch += 1\n",
    "        else:\n",
    "            batch = [load_image(x, self.image_size) for x in self.data_list[self.ptr:self.ptr + batchsize]]\n",
    "            self.ptr += batchsize\n",
    "        \n",
    "        return self.epoch, batch\n",
    "        \n",
    "    def get_pics(self, num):\n",
    "        \"\"\"\n",
    "        取出num张图片，用于快照\n",
    "        不会影响队列\n",
    "        \"\"\"\n",
    "        return np.array([load_image(x, self.image_size) for x in random.sample(self.data_list, num)])\n",
    "\n",
    "def arr2image(X):\n",
    "    \"\"\"\n",
    "    将RGB值从[-1,1]重新转回[0,255]\n",
    "    \"\"\"\n",
    "    int_X = ((X+1)/2*255).clip(0,255).astype('uint8')\n",
    "    return Image.fromarray(int_X)\n",
    "\n",
    "def generate(img, fn):\n",
    "    \"\"\"\n",
    "    将一张图片img送入生成网络fn中\n",
    "    \"\"\"\n",
    "    r = fn([np.array([img])])[0]\n",
    "    return arr2image(np.array(r[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#导入必要的库\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, BatchNormalization, Input, Dropout, Add\n",
    "from keras.layers import Conv2DTranspose, Reshape, Activation, Cropping2D, Flatten\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.activations import relu,tanh\n",
    "from keras.initializers import RandomNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用于初始化\n",
    "conv_init = RandomNormal(0, 0.02)\n",
    "\n",
    "def conv2d(f, *a, **k):\n",
    "    \"\"\"\n",
    "    卷积层\n",
    "    \"\"\"\n",
    "    return Conv2D(f, \n",
    "                  kernel_initializer = conv_init,\n",
    "                  *a, **k)\n",
    "def batchnorm():\n",
    "    \"\"\"\n",
    "    标准化层\n",
    "    \"\"\"\n",
    "    return BatchNormalization(momentum=0.9, epsilon=1.01e-5, axis=-1,)\n",
    "\n",
    "def res_block(x, dim):\n",
    "    \"\"\"\n",
    "    残差网络\n",
    "    [x] --> [卷积] --> [标准化] --> [激活] --> [卷积] --> [标准化] --> [激活] --> [+] --> [激活]\n",
    "     |                                                                        ^\n",
    "     |                                                                        |\n",
    "     +------------------------------------------------------------------------+\n",
    "    \"\"\"\n",
    "    x1 = conv2d(dim, 3, padding=\"same\", use_bias=True)(x)\n",
    "    x1 = batchnorm()(x1, training=1)\n",
    "    x1 = Activation('relu')(x1)\n",
    "    x1 = conv2d(dim, 3, padding=\"same\", use_bias=True)(x1)\n",
    "    x1 = batchnorm()(x1, training=1)\n",
    "    x1 = Activation(\"relu\")(Add()([x,x1]))\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 生成网络\n",
    "我们的生成网络按照三层卷积层、九个残差网络block和三个反卷积层的结构堆叠而成。什么是反卷积层呢？我们知道卷积层具有自己的步长，当步长大于一的时候，输出的尺寸是会小于输入的尺寸的。而反卷积层则相反，当反卷积层的步长大于一的时候，输出的尺寸是大于输入尺寸的。反卷积层可以视作是卷积层的一种逆向操作。它的运算规则和卷积层是相似的。反卷积层首先在输入数据里面填充，使输入的尺寸扩大，然后再用卷积核进行卷积运算，得到的输出尺寸有可能比原本的输入更大。<br />![image.png](https://cdn.nlark.com/yuque/0/2019/png/325286/1556074465774-bc3dbd0f-fa13-490a-84f9-7ca6c6ccf83a.png#align=left&display=inline&height=449&name=image.png&originHeight=449&originWidth=395&size=68961&status=done&width=395)<br /><center>反卷积运算<br />[https://github.com/vdumoulin/conv_arithmetic](https://github.com/vdumoulin/conv_arithmetic)</center><br />有了关于残差网络的知识，这个结构就很容易理解了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NET_G(ngf=64, block_n=6, downsampling_n=2, upsampling_n=2, image_size = 256):\n",
    "    \"\"\"\n",
    "    生成网络\n",
    "    采用resnet结构\n",
    "\n",
    "    block_n为残差网络叠加的数量\n",
    "    论文中采用的参数为 若图片大小为128,采用6；若图片大小为256,采用9\n",
    "\n",
    "    [第一层] 大小为7的卷积核 通道数量 3->ngf \n",
    "    [下采样] 大小为3的卷积核 步长为2 每层通道数量倍增\n",
    "    [残差网络] 九个block叠加\n",
    "    [上采样] \n",
    "    [最后一层] 通道数量变回3\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    input_t = Input(shape=(image_size, image_size, 3))\n",
    "    #输入层\n",
    "\n",
    "    x = input_t\n",
    "    dim = ngf\n",
    "    \n",
    "    x = conv2d(dim, 7, padding=\"same\")(x)\n",
    "    x = batchnorm()(x, training = 1)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    #第一层\n",
    "    \n",
    "    for i in range(downsampling_n):\n",
    "        dim *= 2\n",
    "        x = conv2d(dim, 3, strides = 2, padding=\"same\")(x)\n",
    "        x = batchnorm()(x, training = 1)\n",
    "        x = Activation('relu')(x)\n",
    "    #下采样部分\n",
    "\n",
    "    for i in range(block_n):\n",
    "        x = res_block(x, dim)\n",
    "    #残差网络部分\n",
    "\n",
    "    for i in range(upsampling_n):\n",
    "        dim = dim // 2\n",
    "        x = Conv2DTranspose(dim, 3, strides = 2, kernel_initializer = conv_init, padding=\"same\")(x)\n",
    "        x = batchnorm()(x, training = 1)\n",
    "        x = Activation('relu')(x) \n",
    "    #上采样\n",
    "    \n",
    "    dim = 3\n",
    "    x = conv2d(dim, 7, padding=\"same\")(x)\n",
    "    x = Activation(\"tanh\")(x)\n",
    "    #最后一层\n",
    "    \n",
    "    return Model(inputs=input_t, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 判别网络\n",
    "判别网络的结构比生成网络简单得多，只是几层卷积的叠加而已：\n",
    "\n",
    "\n",
    "\n",
    "在这里，判别网络最后的输出并不是一个数，而是一个矩阵。这并不影响我们对于损失函数的计算，我们只需要把损失函数中的0和1看作是和判别网络具有相同尺寸的矩阵就可以了。<br />这里我们使用了一种新的激活函数，叫做LeakyReLU。这个函数和ReLU非常相似，不同之处只是当x小于0的时候，LeakyReLU的值并不是0,而是仍然有一个较小的斜率。<br />![image.png](https://cdn.nlark.com/yuque/0/2019/png/325286/1556079508777-87362b12-cc3b-4bf7-b9fd-dec7cac64215.png#align=left&display=inline&height=273&name=image.png&originHeight=273&originWidth=704&size=47469&status=done&width=704)<br /><center>ReLU和Leaky ReLU</center><br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NET_D(ndf=64, max_layers = 3, image_size = 256):\n",
    "    \"\"\"\n",
    "    判别网络\n",
    "    \"\"\"\n",
    "    input_t = Input(shape=(image_size, image_size, 3))\n",
    "    \n",
    "    x = input_t\n",
    "    x = conv2d(ndf, 4, padding=\"same\", strides=2)(x)\n",
    "    x = LeakyReLU(alpha = 0.2)(x)\n",
    "    dim = ndf\n",
    "    \n",
    "    for i in range(1, max_layers):\n",
    "        dim *= 2\n",
    "        x = conv2d(dim, 4, padding=\"same\", strides=2, use_bias=False)(x)\n",
    "        x = batchnorm()(x, training=1)\n",
    "        x = LeakyReLU(alpha = 0.2)(x)\n",
    "\n",
    "    x = conv2d(dim, 4, padding=\"same\")(x)\n",
    "    x = batchnorm()(x, training=1)\n",
    "    x = LeakyReLU(alpha = 0.2)(x)\n",
    "        \n",
    "    x = conv2d(1, 4, padding=\"same\", activation = \"sigmoid\")(x)\n",
    "    return Model(inputs=input_t, outputs=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(output, target):\n",
    "    \"\"\"\n",
    "    损失函数\n",
    "    论文中提到使用平方损失更好\n",
    "    \"\"\"\n",
    "    return K.mean(K.abs(K.square(output-target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 网络结构的搭建\n",
    "我们采用“类”的概念来组织GAN的网络结构："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGAN(object):\n",
    "    def __init__(self, image_size=256, lambda_cyc=10, lrD = 2e-4, lrG = 2e-4, ndf = 64, ngf = 64, resnet_blocks = 9):\n",
    "        \"\"\"\n",
    "        构建网络结构\n",
    "                      cyc loss\n",
    "         +---------------------------------+      \n",
    "         |            (CycleA)             |       \n",
    "         v                                 |\n",
    "        realA -> [GB] -> fakeB -> [GA] -> recA          \n",
    "         |                 |\n",
    "         |                 +---------------+\n",
    "         |                                 |\n",
    "         v                                 v\n",
    "        [DA]         <CycleGAN>           [DB]\n",
    "         ^                                 ^\n",
    "         |                                 |\n",
    "         +----------------+                |\n",
    "                          |                |\n",
    "        recB <- [GB] <- fakeA <- [GA] <- realB          \n",
    "         |                                 ^\n",
    "         |            (CycleB)             |\n",
    "         +---------------------------------+\n",
    "                        cyc loss\n",
    "        \"\"\"\n",
    "        \n",
    "        #创建生成网络\n",
    "        self.GA = NET_G(image_size = image_size, ngf = ngf, block_n = resnet_blocks)\n",
    "        self.GB = NET_G(image_size = image_size, ngf = ngf, block_n = resnet_blocks)\n",
    "        \n",
    "        #创建判别网络\n",
    "        self.DA = NET_D(image_size = image_size, ndf = ndf)\n",
    "        self.DB = NET_D(image_size = image_size, ndf = ndf)\n",
    "\n",
    "        #获取真实、伪造和复原的A类图和B类图变量\n",
    "        realA, realB = self.GB.inputs[0],  self.GA.inputs[0]\n",
    "        fakeB, fakeA = self.GB.outputs[0], self.GA.outputs[0]\n",
    "        recA,  recB  = self.GA([fakeB]),   self.GB([fakeA])\n",
    "\n",
    "        #获取由真实图片生成伪造图片和复原图片的函数\n",
    "        self.cycleA = K.function([realA], [fakeB,recA])\n",
    "        self.cycleB = K.function([realB], [fakeA,recB])\n",
    "\n",
    "        #获得判别网络判别真实图片和伪造图片的结果\n",
    "        DrealA, DrealB = self.DA([realA]), self.DB([realB])\n",
    "        DfakeA, DfakeB = self.DA([fakeA]), self.DB([fakeB])\n",
    "\n",
    "        #用生成网络和判别网络的结果计算损失函数\n",
    "        lossDA, lossGA, lossCycA = self.get_loss(DrealA, DfakeA, realA, recA)\n",
    "        lossDB, lossGB, lossCycB = self.get_loss(DrealB, DfakeB, realB, recB)\n",
    "\n",
    "        lossG = lossGA + lossGB + lambda_cyc * (lossCycA + lossCycB)\n",
    "        lossD = lossDA + lossDB\n",
    "\n",
    "        #获取参数更新器\n",
    "        updaterG = Adam(lr = lrG, beta_1=0.5).get_updates(self.GA.trainable_weights + self.GB.trainable_weights, [], lossG)\n",
    "        updaterD = Adam(lr = lrD, beta_1=0.5).get_updates(self.DA.trainable_weights + self.DB.trainable_weights, [], lossD)\n",
    "        \n",
    "        #创建训练函数，可以通过调用这两个函数来训练网络\n",
    "        self.trainG = K.function([realA, realB], [lossGA, lossGB, lossCycA, lossCycB], updaterG)\n",
    "        self.trainD = K.function([realA, realB], [lossDA, lossDB], updaterD)\n",
    "    \n",
    "    \n",
    "    def get_loss(self, Dreal, Dfake, real , rec):\n",
    "        \"\"\"\n",
    "        获取网络中的损失函数\n",
    "        \"\"\"\n",
    "        lossD = loss_func(Dreal, K.ones_like(Dreal)) + loss_func(Dfake, K.zeros_like(Dfake))\n",
    "        lossG = loss_func(Dfake, K.ones_like(Dfake))\n",
    "        lossCyc = K.mean(K.abs(real - rec))\n",
    "        return lossD, lossG, lossCyc\n",
    "    \n",
    "    def save(self, path=\"./models/model\"):\n",
    "        self.GA.save(\"{}-GA.h5\".format(path))\n",
    "        self.GB.save(\"{}-GB.h5\".format(path))\n",
    "        self.DA.save(\"{}-DA.h5\".format(path))\n",
    "        self.DB.save(\"{}-DB.h5\".format(path))\n",
    "\n",
    "    def train(self, A, B):\n",
    "        errDA, errDB = self.trainD([A, B])\n",
    "        errGA, errGB, errCycA, errCycB = self.trainG([A, B])\n",
    "        return errDA, errDB, errGA, errGB, errCycA, errCycB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练相关代码\n",
    "\n",
    "在这里，我们提供了一个snapshot函数，可以在训练的过程中生成预览效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入神经网络的图片尺寸\n",
    "IMG_SIZE = 128\n",
    "\n",
    "#数据集名称\n",
    "DATASET = \"vangogh2photo\"\n",
    "\n",
    "#数据集路径\n",
    "dataset_path = \"./data/{}/\".format(DATASET)\n",
    "\n",
    "\n",
    "trainA_path = dataset_path + \"trainA/*.jpg\"\n",
    "trainB_path = dataset_path + \"trainB/*.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_A = DataSet(trainA_path, image_size = IMG_SIZE)\n",
    "train_B = DataSet(trainB_path, image_size = IMG_SIZE)\n",
    "\n",
    "def train_batch(batchsize):\n",
    "    \"\"\"\n",
    "    从数据集中取出一个Batch\n",
    "    \"\"\"\n",
    "    epa, a = train_A.get_batch(batchsize)\n",
    "    epb, b = train_B.get_batch(batchsize)\n",
    "    return max(epa, epb), a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(generator, X):\n",
    "    r = np.array([generator([np.array([x])]) for x in X])\n",
    "    g = r[:,0,0]\n",
    "    rec = r[:,1,0]\n",
    "    return g, rec \n",
    "\n",
    "def snapshot(cycleA, cycleB, A, B):        \n",
    "    \"\"\"\n",
    "    产生一个快照\n",
    "    \n",
    "    A、B是两个图片列表\n",
    "    cycleA是 A->B->A的一个循环\n",
    "    cycleB是 B->A->B的一个循环\n",
    "    \n",
    "    输出一张图片：\n",
    "    +-----------+     +-----------+\n",
    "    | X (in A)  | ... |  Y (in B) | ...\n",
    "    +-----------+     +-----------+\n",
    "    |   GB(X)   | ... |   GA(Y)   | ...\n",
    "    +-----------+     +-----------+\n",
    "    | GA(GB(X)) | ... | GB(GA(Y)) | ...\n",
    "    +-----------+     +-----------+\n",
    "    \"\"\"\n",
    "    gA, recA = gen(cycleA, A)\n",
    "    gB, recB = gen(cycleB, B)\n",
    "\n",
    "    lines = [\n",
    "        np.concatenate(A.tolist()+B.tolist(), axis = 1),\n",
    "        np.concatenate(gA.tolist()+gB.tolist(), axis = 1),\n",
    "        np.concatenate(recA.tolist()+recB.tolist(), axis = 1)\n",
    "    ]\n",
    "\n",
    "    arr = np.concatenate(lines)\n",
    "    return arr2image(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建模型\n",
    "model = CycleGAN(image_size = IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[迭代5] 判别损失: A 0.553110 B 0.544421 生成损失: A 0.328847 B 0.338626 循环损失: A 0.540241 B 0.541242\n",
      "[迭代10] 判别损失: A 0.504546 B 0.516119 生成损失: A 0.383115 B 0.360146 循环损失: A 0.416725 B 0.459113\n",
      "[迭代15] 判别损失: A 0.493387 B 0.509429 生成损失: A 0.374185 B 0.357337 循环损失: A 0.523256 B 0.368838\n",
      "[迭代20] 判别损失: A 0.398151 B 0.456671 生成损失: A 0.437630 B 0.377726 循环损失: A 0.392315 B 0.376411\n",
      "[迭代25] 判别损失: A 0.445816 B 0.486390 生成损失: A 0.425615 B 0.403395 循环损失: A 0.400680 B 0.397654\n",
      "[迭代30] 判别损失: A 0.474137 B 0.487787 生成损失: A 0.404137 B 0.364058 循环损失: A 0.343778 B 0.443276\n",
      "[迭代35] 判别损失: A 0.430336 B 0.465811 生成损失: A 0.393310 B 0.362208 循环损失: A 0.296051 B 0.370275\n",
      "[迭代40] 判别损失: A 0.502109 B 0.450251 生成损失: A 0.415723 B 0.392026 循环损失: A 0.317308 B 0.463644\n",
      "[迭代45] 判别损失: A 0.499073 B 0.427926 生成损失: A 0.374012 B 0.405073 循环损失: A 0.317396 B 0.420848\n",
      "[迭代50] 判别损失: A 0.384761 B 0.414583 生成损失: A 0.426253 B 0.396444 循环损失: A 0.327207 B 0.320688\n",
      "[迭代55] 判别损失: A 0.405843 B 0.436911 生成损失: A 0.435845 B 0.383412 循环损失: A 0.354497 B 0.284417\n",
      "[迭代60] 判别损失: A 0.398623 B 0.366905 生成损失: A 0.450182 B 0.456226 循环损失: A 0.301355 B 0.428636\n",
      "[迭代65] 判别损失: A 0.366220 B 0.361681 生成损失: A 0.477452 B 0.452700 循环损失: A 0.311807 B 0.452349\n",
      "[迭代70] 判别损失: A 0.295299 B 0.369329 生成损失: A 0.530767 B 0.441368 循环损失: A 0.324492 B 0.261953\n",
      "[迭代75] 判别损失: A 0.400175 B 0.494933 生成损失: A 0.415176 B 0.416072 循环损失: A 0.316468 B 0.342542\n",
      "[迭代80] 判别损失: A 0.421349 B 0.402838 生成损失: A 0.440050 B 0.391341 循环损失: A 0.316445 B 0.293394\n",
      "[迭代85] 判别损失: A 0.415708 B 0.431271 生成损失: A 0.436332 B 0.432433 循环损失: A 0.309313 B 0.394961\n",
      "[迭代90] 判别损失: A 0.359592 B 0.448819 生成损失: A 0.492292 B 0.370408 循环损失: A 0.321227 B 0.230084\n",
      "[迭代95] 判别损失: A 0.393738 B 0.390251 生成损失: A 0.445943 B 0.422957 循环损失: A 0.272716 B 0.439101\n",
      "[迭代100] 判别损失: A 0.398116 B 0.414719 生成损失: A 0.437445 B 0.443710 循环损失: A 0.313588 B 0.324370\n",
      "[迭代105] 判别损失: A 0.505513 B 0.435528 生成损失: A 0.352828 B 0.465521 循环损失: A 0.342923 B 0.247871\n",
      "[迭代110] 判别损失: A 0.498755 B 0.346103 生成损失: A 0.415411 B 0.453058 循环损失: A 0.374802 B 0.386089\n",
      "[迭代115] 判别损失: A 0.379372 B 0.403217 生成损失: A 0.448058 B 0.394248 循环损失: A 0.307981 B 0.325358\n",
      "[迭代120] 判别损失: A 0.364490 B 0.442732 生成损失: A 0.445645 B 0.430798 循环损失: A 0.354506 B 0.366225\n",
      "[迭代125] 判别损失: A 0.280051 B 0.388086 生成损失: A 0.482386 B 0.423858 循环损失: A 0.307772 B 0.348008\n",
      "[迭代130] 判别损失: A 0.503510 B 0.378313 生成损失: A 0.383435 B 0.420659 循环损失: A 0.308677 B 0.320587\n",
      "[迭代135] 判别损失: A 0.440360 B 0.392787 生成损失: A 0.425904 B 0.403980 循环损失: A 0.366939 B 0.321608\n",
      "[迭代140] 判别损失: A 0.408965 B 0.405940 生成损失: A 0.430949 B 0.394076 循环损失: A 0.371869 B 0.350501\n",
      "[迭代145] 判别损失: A 0.339787 B 0.462226 生成损失: A 0.456471 B 0.403663 循环损失: A 0.334176 B 0.366151\n",
      "[迭代150] 判别损失: A 0.363361 B 0.542631 生成损失: A 0.438423 B 0.409301 循环损失: A 0.333406 B 0.299055\n",
      "[迭代155] 判别损失: A 0.395871 B 0.512518 生成损失: A 0.448443 B 0.337439 循环损失: A 0.309152 B 0.316866\n",
      "[迭代160] 判别损失: A 0.364359 B 0.430450 生成损失: A 0.444278 B 0.369667 循环损失: A 0.367814 B 0.368179\n",
      "[迭代165] 判别损失: A 0.387262 B 0.531849 生成损失: A 0.435006 B 0.346778 循环损失: A 0.345119 B 0.305830\n",
      "[迭代170] 判别损失: A 0.409058 B 0.430151 生成损失: A 0.486454 B 0.392772 循环损失: A 0.290101 B 0.296911\n",
      "[迭代175] 判别损失: A 0.339903 B 0.536575 生成损失: A 0.529468 B 0.388582 循环损失: A 0.306833 B 0.303231\n",
      "[迭代180] 判别损失: A 0.472891 B 0.383777 生成损失: A 0.487170 B 0.410080 循环损失: A 0.278160 B 0.358924\n",
      "[迭代185] 判别损失: A 0.436348 B 0.478136 生成损失: A 0.441741 B 0.357955 循环损失: A 0.306344 B 0.346604\n",
      "[迭代190] 判别损失: A 0.359817 B 0.447438 生成损失: A 0.446663 B 0.382217 循环损失: A 0.277643 B 0.273858\n",
      "[迭代195] 判别损失: A 0.263272 B 0.380580 生成损失: A 0.556862 B 0.403015 循环损失: A 0.237027 B 0.387146\n",
      "[迭代200] 判别损失: A 0.416123 B 0.287564 生成损失: A 0.494355 B 0.492098 循环损失: A 0.261706 B 0.389915\n",
      "[迭代205] 判别损失: A 0.296656 B 0.329758 生成损失: A 0.549457 B 0.434056 循环损失: A 0.357315 B 0.310869\n",
      "[迭代210] 判别损失: A 0.325304 B 0.404441 生成损失: A 0.572382 B 0.448267 循环损失: A 0.365512 B 0.272495\n",
      "[迭代215] 判别损失: A 0.318916 B 0.373669 生成损失: A 0.517537 B 0.565932 循环损失: A 0.370467 B 0.314548\n",
      "[迭代220] 判别损失: A 0.294586 B 0.339865 生成损失: A 0.566355 B 0.485037 循环损失: A 0.309217 B 0.370344\n",
      "[迭代225] 判别损失: A 0.388776 B 0.455452 生成损失: A 0.521351 B 0.413997 循环损失: A 0.271931 B 0.314216\n",
      "[迭代230] 判别损失: A 0.365921 B 0.541234 生成损失: A 0.564293 B 0.325369 循环损失: A 0.272759 B 0.205691\n",
      "[迭代235] 判别损失: A 0.339366 B 0.454086 生成损失: A 0.503683 B 0.424261 循环损失: A 0.257867 B 0.305872\n",
      "[迭代240] 判别损失: A 0.311513 B 0.396880 生成损失: A 0.581687 B 0.432180 循环损失: A 0.333711 B 0.371175\n",
      "[迭代245] 判别损失: A 0.347374 B 0.370687 生成损失: A 0.504555 B 0.491593 循环损失: A 0.280664 B 0.385797\n",
      "[迭代250] 判别损失: A 0.396538 B 0.337694 生成损失: A 0.521479 B 0.469674 循环损失: A 0.293951 B 0.311192\n",
      "[迭代255] 判别损失: A 0.248978 B 0.390406 生成损失: A 0.605258 B 0.441120 循环损失: A 0.337044 B 0.423676\n",
      "[迭代260] 判别损失: A 0.297240 B 0.317871 生成损失: A 0.582142 B 0.466093 循环损失: A 0.246713 B 0.268525\n",
      "[迭代265] 判别损失: A 0.362353 B 0.311698 生成损失: A 0.621386 B 0.515616 循环损失: A 0.309510 B 0.418466\n",
      "[迭代270] 判别损失: A 0.319128 B 0.448953 生成损失: A 0.593586 B 0.484768 循环损失: A 0.303816 B 0.335220\n",
      "[迭代275] 判别损失: A 0.629936 B 0.399695 生成损失: A 0.315269 B 0.473723 循环损失: A 0.276775 B 0.308741\n",
      "[迭代280] 判别损失: A 0.396498 B 0.404591 生成损失: A 0.581448 B 0.438118 循环损失: A 0.263721 B 0.301654\n",
      "[迭代285] 判别损失: A 0.185653 B 0.289599 生成损失: A 0.698074 B 0.523391 循环损失: A 0.262326 B 0.226459\n",
      "[迭代290] 判别损失: A 0.360555 B 0.378025 生成损失: A 0.557965 B 0.453943 循环损失: A 0.272345 B 0.400178\n",
      "[迭代295] 判别损失: A 0.326434 B 0.310967 生成损失: A 0.602949 B 0.484104 循环损失: A 0.292342 B 0.336661\n",
      "[迭代300] 判别损失: A 0.225158 B 0.328639 生成损失: A 0.657401 B 0.557959 循环损失: A 0.264406 B 0.367199\n",
      "[迭代305] 判别损失: A 0.318969 B 0.371530 生成损失: A 0.576859 B 0.469404 循环损失: A 0.320865 B 0.325316\n",
      "[迭代310] 判别损失: A 0.372756 B 0.390986 生成损失: A 0.562131 B 0.550409 循环损失: A 0.262814 B 0.309530\n",
      "[迭代315] 判别损失: A 0.356624 B 0.454010 生成损失: A 0.554124 B 0.410001 循环损失: A 0.286854 B 0.287778\n",
      "[迭代320] 判别损失: A 0.189386 B 0.416664 生成损失: A 0.683966 B 0.382394 循环损失: A 0.294769 B 0.271239\n",
      "[迭代325] 判别损失: A 0.259576 B 0.450899 生成损失: A 0.686024 B 0.481905 循环损失: A 0.255536 B 0.222150\n",
      "[迭代330] 判别损失: A 0.223950 B 0.247411 生成损失: A 0.606316 B 0.530652 循环损失: A 0.252329 B 0.380532\n",
      "[迭代335] 判别损失: A 0.378693 B 0.237343 生成损失: A 0.589345 B 0.521417 循环损失: A 0.320628 B 0.337770\n",
      "[迭代340] 判别损失: A 0.348596 B 0.283860 生成损失: A 0.539438 B 0.517175 循环损失: A 0.275666 B 0.285079\n",
      "[迭代345] 判别损失: A 0.373622 B 0.266887 生成损失: A 0.603299 B 0.588367 循环损失: A 0.305806 B 0.291538\n",
      "[迭代350] 判别损失: A 0.286102 B 0.215859 生成损失: A 0.649995 B 0.548702 循环损失: A 0.258462 B 0.238834\n",
      "[迭代355] 判别损失: A 0.410035 B 0.254391 生成损失: A 0.569833 B 0.590975 循环损失: A 0.276804 B 0.317875\n",
      "[迭代360] 判别损失: A 0.593304 B 0.321025 生成损失: A 0.492926 B 0.582303 循环损失: A 0.259264 B 0.392944\n",
      "[迭代365] 判别损失: A 0.565918 B 0.110507 生成损失: A 0.406892 B 0.646123 循环损失: A 0.287133 B 0.359297\n",
      "[迭代370] 判别损失: A 0.308721 B 0.121877 生成损失: A 0.574873 B 0.642370 循环损失: A 0.270987 B 0.262551\n",
      "[迭代375] 判别损失: A 0.328581 B 0.253361 生成损失: A 0.598918 B 0.651231 循环损失: A 0.279350 B 0.317579\n",
      "[迭代380] 判别损失: A 0.335894 B 0.188826 生成损失: A 0.671365 B 0.610948 循环损失: A 0.344400 B 0.341863\n",
      "[迭代385] 判别损失: A 0.445632 B 0.330097 生成损失: A 0.535842 B 0.614660 循环损失: A 0.255665 B 0.301950\n",
      "[迭代390] 判别损失: A 0.406308 B 0.290112 生成损失: A 0.587400 B 0.584427 循环损失: A 0.325128 B 0.303148\n",
      "[迭代395] 判别损失: A 0.325302 B 0.339212 生成损失: A 0.607796 B 0.550574 循环损失: A 0.302404 B 0.273816\n",
      "[迭代400] 判别损失: A 0.335671 B 0.314121 生成损失: A 0.603005 B 0.551954 循环损失: A 0.280302 B 0.376280\n",
      "[迭代405] 判别损失: A 0.255123 B 0.512448 生成损失: A 0.633637 B 0.483195 循环损失: A 0.290680 B 0.263511\n",
      "[迭代410] 判别损失: A 0.330845 B 0.372738 生成损失: A 0.656766 B 0.487714 循环损失: A 0.277613 B 0.228541\n",
      "[迭代415] 判别损失: A 0.227489 B 0.373973 生成损失: A 0.643738 B 0.522216 循环损失: A 0.281586 B 0.235297\n",
      "[迭代420] 判别损失: A 0.198679 B 0.457490 生成损失: A 0.642116 B 0.495478 循环损失: A 0.298818 B 0.295624\n",
      "[迭代425] 判别损失: A 0.196301 B 0.474479 生成损失: A 0.691460 B 0.420579 循环损失: A 0.254870 B 0.267003\n",
      "[迭代430] 判别损失: A 0.236822 B 0.532341 生成损失: A 0.666373 B 0.436039 循环损失: A 0.285175 B 0.305930\n",
      "[迭代435] 判别损失: A 0.314304 B 0.543178 生成损失: A 0.648995 B 0.412913 循环损失: A 0.290147 B 0.240666\n",
      "[迭代440] 判别损失: A 0.421788 B 0.365574 生成损失: A 0.535751 B 0.489644 循环损失: A 0.285934 B 0.308800\n",
      "[迭代445] 判别损失: A 0.452524 B 0.266637 生成损失: A 0.596653 B 0.563104 循环损失: A 0.328669 B 0.405769\n",
      "[迭代450] 判别损失: A 0.242712 B 0.527368 生成损失: A 0.678777 B 0.429729 循环损失: A 0.281509 B 0.296514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[迭代455] 判别损失: A 0.169861 B 0.500008 生成损失: A 0.711304 B 0.485119 循环损失: A 0.242523 B 0.323943\n",
      "[迭代460] 判别损失: A 0.307695 B 0.550725 生成损失: A 0.693025 B 0.486315 循环损失: A 0.266561 B 0.262663\n",
      "[迭代465] 判别损失: A 0.196316 B 0.345400 生成损失: A 0.680734 B 0.544456 循环损失: A 0.266458 B 0.359906\n",
      "[迭代470] 判别损失: A 0.179850 B 0.565964 生成损失: A 0.724033 B 0.413497 循环损失: A 0.275015 B 0.364905\n",
      "[迭代475] 判别损失: A 0.394354 B 0.502446 生成损失: A 0.599829 B 0.563800 循环损失: A 0.289285 B 0.385139\n",
      "[迭代480] 判别损失: A 0.290515 B 0.352812 生成损失: A 0.597007 B 0.475130 循环损失: A 0.320142 B 0.286595\n",
      "[迭代485] 判别损失: A 0.359308 B 0.379783 生成损失: A 0.582148 B 0.481284 循环损失: A 0.311575 B 0.224642\n",
      "[迭代490] 判别损失: A 0.311529 B 0.276052 生成损失: A 0.570040 B 0.518140 循环损失: A 0.283852 B 0.303468\n",
      "[迭代495] 判别损失: A 0.354695 B 0.339651 生成损失: A 0.604951 B 0.559308 循环损失: A 0.268436 B 0.277632\n",
      "[迭代500] 判别损失: A 0.422122 B 0.257715 生成损失: A 0.694205 B 0.573981 循环损失: A 0.319458 B 0.401114\n",
      "[迭代505] 判别损失: A 0.462241 B 0.323628 生成损失: A 0.456074 B 0.510663 循环损失: A 0.282339 B 0.338383\n",
      "[迭代510] 判别损失: A 0.366029 B 0.429561 生成损失: A 0.560299 B 0.479508 循环损失: A 0.337780 B 0.212124\n",
      "[迭代515] 判别损失: A 0.368986 B 0.280499 生成损失: A 0.663024 B 0.588548 循环损失: A 0.323258 B 0.320871\n",
      "[迭代520] 判别损失: A 0.509599 B 0.287154 生成损失: A 0.499187 B 0.479088 循环损失: A 0.265050 B 0.236183\n",
      "[迭代525] 判别损失: A 0.523405 B 0.348714 生成损失: A 0.518416 B 0.534540 循环损失: A 0.259355 B 0.371917\n",
      "[迭代530] 判别损失: A 0.559915 B 0.262505 生成损失: A 0.534288 B 0.699204 循环损失: A 0.333864 B 0.368597\n",
      "[迭代535] 判别损失: A 0.349871 B 0.249221 生成损失: A 0.583553 B 0.549572 循环损失: A 0.247756 B 0.403848\n",
      "[迭代540] 判别损失: A 0.482247 B 0.256007 生成损失: A 0.568003 B 0.620410 循环损失: A 0.275356 B 0.302707\n",
      "[迭代545] 判别损失: A 0.293723 B 0.522770 生成损失: A 0.616993 B 0.556454 循环损失: A 0.271914 B 0.223411\n",
      "[迭代550] 判别损失: A 0.495931 B 0.383930 生成损失: A 0.510048 B 0.478069 循环损失: A 0.234901 B 0.279764\n",
      "[迭代555] 判别损失: A 0.395421 B 0.297872 生成损失: A 0.539725 B 0.569086 循环损失: A 0.253536 B 0.363426\n",
      "[迭代560] 判别损失: A 0.317121 B 0.157349 生成损失: A 0.515550 B 0.624355 循环损失: A 0.272909 B 0.220417\n",
      "[迭代565] 判别损失: A 0.333657 B 0.225585 生成损失: A 0.554922 B 0.629922 循环损失: A 0.281600 B 0.263936\n",
      "[迭代570] 判别损失: A 0.445355 B 0.108571 生成损失: A 0.480037 B 0.684885 循环损失: A 0.277211 B 0.287698\n",
      "[迭代575] 判别损失: A 0.493457 B 0.084928 生成损失: A 0.447547 B 0.717089 循环损失: A 0.274976 B 0.334713\n",
      "[迭代580] 判别损失: A 0.422714 B 0.038589 生成损失: A 0.541194 B 0.776868 循环损失: A 0.297755 B 0.262257\n",
      "[迭代585] 判别损失: A 0.420046 B 0.054767 生成损失: A 0.569988 B 0.757821 循环损失: A 0.247783 B 0.310449\n",
      "[迭代590] 判别损失: A 0.284479 B 0.254597 生成损失: A 0.660675 B 0.601584 循环损失: A 0.277640 B 0.235891\n",
      "[迭代595] 判别损失: A 0.549838 B 0.179480 生成损失: A 0.589783 B 0.682552 循环损失: A 0.242144 B 0.278324\n",
      "[迭代600] 判别损失: A 0.530126 B 0.355958 生成损失: A 0.500644 B 0.604059 循环损失: A 0.341027 B 0.239468\n",
      "[迭代605] 判别损失: A 0.496812 B 0.440606 生成损失: A 0.567310 B 0.575460 循环损失: A 0.317496 B 0.290545\n",
      "[迭代610] 判别损失: A 0.245149 B 0.248419 生成损失: A 0.645928 B 0.660521 循环损失: A 0.273883 B 0.346944\n",
      "[迭代615] 判别损失: A 0.125066 B 0.412321 生成损失: A 0.744128 B 0.570440 循环损失: A 0.320814 B 0.421576\n",
      "[迭代620] 判别损失: A 0.189140 B 0.657227 生成损失: A 0.695653 B 0.498948 循环损失: A 0.330940 B 0.268886\n",
      "[迭代625] 判别损失: A 0.102182 B 0.570338 生成损失: A 0.698522 B 0.485030 循环损失: A 0.281612 B 0.354550\n",
      "[迭代630] 判别损失: A 0.257271 B 0.293162 生成损失: A 0.636283 B 0.598291 循环损失: A 0.331455 B 0.281422\n",
      "[迭代635] 判别损失: A 0.226456 B 0.351255 生成损失: A 0.732477 B 0.549591 循环损失: A 0.302633 B 0.256187\n",
      "[迭代640] 判别损失: A 0.225790 B 0.243144 生成损失: A 0.686988 B 0.623979 循环损失: A 0.261353 B 0.304916\n",
      "[迭代645] 判别损失: A 0.317006 B 0.294452 生成损失: A 0.574914 B 0.573959 循环损失: A 0.300446 B 0.250240\n",
      "[迭代650] 判别损失: A 0.575854 B 0.285150 生成损失: A 0.513806 B 0.641739 循环损失: A 0.247662 B 0.334527\n",
      "[迭代655] 判别损失: A 0.278474 B 0.319678 生成损失: A 0.581060 B 0.666961 循环损失: A 0.283478 B 0.300425\n",
      "[迭代660] 判别损失: A 0.260729 B 0.325392 生成损失: A 0.692502 B 0.551195 循环损失: A 0.318209 B 0.222805\n",
      "[迭代665] 判别损失: A 0.360871 B 0.454836 生成损失: A 0.601442 B 0.465555 循环损失: A 0.297059 B 0.366351\n",
      "[迭代670] 判别损失: A 0.468501 B 0.294247 生成损失: A 0.429885 B 0.598173 循环损失: A 0.219779 B 0.272562\n",
      "[迭代675] 判别损失: A 0.601165 B 0.385186 生成损失: A 0.387689 B 0.589071 循环损失: A 0.245660 B 0.355547\n",
      "[迭代680] 判别损失: A 0.346192 B 0.281806 生成损失: A 0.591043 B 0.647288 循环损失: A 0.258714 B 0.266416\n",
      "[迭代685] 判别损失: A 0.379822 B 0.411155 生成损失: A 0.536517 B 0.531729 循环损失: A 0.245491 B 0.231299\n",
      "[迭代690] 判别损失: A 0.368046 B 0.361881 生成损失: A 0.507183 B 0.580844 循环损失: A 0.281218 B 0.219132\n",
      "[迭代695] 判别损失: A 0.633218 B 0.351680 生成损失: A 0.450584 B 0.554807 循环损失: A 0.210739 B 0.371871\n",
      "[迭代700] 判别损失: A 0.377753 B 0.324689 生成损失: A 0.556310 B 0.552056 循环损失: A 0.253740 B 0.306589\n",
      "[迭代705] 判别损失: A 0.407810 B 0.549017 生成损失: A 0.635423 B 0.598816 循环损失: A 0.298659 B 0.280518\n",
      "[迭代710] 判别损失: A 0.516612 B 0.331202 生成损失: A 0.536247 B 0.545020 循环损失: A 0.260447 B 0.298847\n",
      "[迭代715] 判别损失: A 0.513501 B 0.356742 生成损失: A 0.502589 B 0.547113 循环损失: A 0.298809 B 0.235397\n",
      "[迭代720] 判别损失: A 0.435085 B 0.299080 生成损失: A 0.440978 B 0.592270 循环损失: A 0.230165 B 0.299000\n",
      "[迭代725] 判别损失: A 0.563585 B 0.311390 生成损失: A 0.488417 B 0.567433 循环损失: A 0.219490 B 0.238837\n",
      "[迭代730] 判别损失: A 0.290001 B 0.087095 生成损失: A 0.534764 B 0.677298 循环损失: A 0.288178 B 0.217555\n",
      "[迭代735] 判别损失: A 0.275794 B 0.213644 生成损失: A 0.606985 B 0.643060 循环损失: A 0.314047 B 0.283656\n",
      "[迭代740] 判别损失: A 0.281409 B 0.283952 生成损失: A 0.601261 B 0.602433 循环损失: A 0.322859 B 0.227696\n",
      "[迭代745] 判别损失: A 0.319773 B 0.210002 生成损失: A 0.570340 B 0.665788 循环损失: A 0.249535 B 0.313585\n",
      "[迭代750] 判别损失: A 0.384949 B 0.504861 生成损失: A 0.565215 B 0.481647 循环损失: A 0.280645 B 0.260326\n",
      "[迭代755] 判别损失: A 0.228368 B 0.327343 生成损失: A 0.705766 B 0.514553 循环损失: A 0.296226 B 0.269933\n",
      "[迭代760] 判别损失: A 0.413892 B 0.202982 生成损失: A 0.654012 B 0.623215 循环损失: A 0.275384 B 0.347849\n",
      "[迭代765] 判别损失: A 0.563507 B 0.192808 生成损失: A 0.467435 B 0.637694 循环损失: A 0.258866 B 0.346211\n",
      "[迭代770] 判别损失: A 0.259207 B 0.510864 生成损失: A 0.649926 B 0.513921 循环损失: A 0.276288 B 0.233497\n",
      "[迭代775] 判别损失: A 0.418633 B 0.219193 生成损失: A 0.525035 B 0.653049 循环损失: A 0.321781 B 0.271928\n",
      "[迭代780] 判别损失: A 0.363663 B 0.293726 生成损失: A 0.660645 B 0.625842 循环损失: A 0.338018 B 0.351307\n",
      "[迭代785] 判别损失: A 0.629115 B 0.381475 生成损失: A 0.407517 B 0.521579 循环损失: A 0.291117 B 0.291720\n",
      "[迭代790] 判别损失: A 0.408129 B 0.252900 生成损失: A 0.515394 B 0.722318 循环损失: A 0.306418 B 0.321016\n",
      "[迭代795] 判别损失: A 0.381218 B 0.268091 生成损失: A 0.568490 B 0.614649 循环损失: A 0.212769 B 0.350827\n",
      "[迭代800] 判别损失: A 0.773771 B 0.298270 生成损失: A 0.288880 B 0.605541 循环损失: A 0.232927 B 0.247786\n",
      "[迭代805] 判别损失: A 0.423487 B 0.486437 生成损失: A 0.539005 B 0.465236 循环损失: A 0.240999 B 0.316615\n",
      "[迭代810] 判别损失: A 0.403166 B 0.371364 生成损失: A 0.544051 B 0.615754 循环损失: A 0.272814 B 0.324285\n",
      "[迭代815] 判别损失: A 0.412510 B 0.565969 生成损失: A 0.538074 B 0.512939 循环损失: A 0.262398 B 0.182257\n",
      "[迭代820] 判别损失: A 0.485722 B 0.681678 生成损失: A 0.470055 B 0.467626 循环损失: A 0.276691 B 0.240623\n",
      "[迭代825] 判别损失: A 0.597857 B 0.488240 生成损失: A 0.384153 B 0.607027 循环损失: A 0.281906 B 0.361158\n",
      "[迭代830] 判别损失: A 0.529151 B 0.552829 生成损失: A 0.404749 B 0.565789 循环损失: A 0.240607 B 0.352271\n",
      "[迭代835] 判别损失: A 0.414929 B 0.509880 生成损失: A 0.437162 B 0.577454 循环损失: A 0.242688 B 0.290072\n",
      "[迭代840] 判别损失: A 0.443587 B 0.441899 生成损失: A 0.441823 B 0.591488 循环损失: A 0.248674 B 0.234408\n",
      "[迭代845] 判别损失: A 0.396850 B 0.622772 生成损失: A 0.462019 B 0.468395 循环损失: A 0.274038 B 0.227149\n",
      "[迭代850] 判别损失: A 0.365872 B 0.400034 生成损失: A 0.493141 B 0.503545 循环损失: A 0.210071 B 0.333639\n",
      "[迭代855] 判别损失: A 0.359783 B 0.407011 生成损失: A 0.647596 B 0.493667 循环损失: A 0.212667 B 0.258730\n",
      "[迭代860] 判别损失: A 0.672421 B 0.403085 生成损失: A 0.435872 B 0.528264 循环损失: A 0.256486 B 0.295426\n",
      "[迭代865] 判别损失: A 0.425303 B 0.364037 生成损失: A 0.478289 B 0.493571 循环损失: A 0.223488 B 0.217386\n",
      "[迭代870] 判别损失: A 0.447216 B 0.344048 生成损失: A 0.532704 B 0.553620 循环损失: A 0.262446 B 0.263808\n",
      "[迭代875] 判别损失: A 0.633887 B 0.163739 生成损失: A 0.517403 B 0.618366 循环损失: A 0.216792 B 0.254973\n",
      "[迭代880] 判别损失: A 0.410285 B 0.054055 生成损失: A 0.490585 B 0.737132 循环损失: A 0.238262 B 0.264203\n",
      "[迭代885] 判别损失: A 0.473278 B 0.034503 生成损失: A 0.534372 B 0.784612 循环损失: A 0.225467 B 0.442917\n",
      "[迭代890] 判别损失: A 0.245136 B 0.102221 生成损失: A 0.629642 B 0.703245 循环损失: A 0.338565 B 0.226245\n",
      "[迭代895] 判别损失: A 0.411316 B 0.164554 生成损失: A 0.517040 B 0.647860 循环损失: A 0.273190 B 0.301638\n",
      "[迭代900] 判别损失: A 0.454108 B 0.074695 生成损失: A 0.497610 B 0.752902 循环损失: A 0.238408 B 0.274112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[迭代905] 判别损失: A 0.374640 B 0.126849 生成损失: A 0.513107 B 0.687610 循环损失: A 0.320363 B 0.275902\n",
      "[迭代910] 判别损失: A 0.619684 B 0.417085 生成损失: A 0.526699 B 0.581088 循环损失: A 0.312578 B 0.237803\n",
      "[迭代915] 判别损失: A 0.410815 B 0.196721 生成损失: A 0.438679 B 0.662643 循环损失: A 0.218231 B 0.311470\n",
      "[迭代920] 判别损失: A 0.461322 B 0.291681 生成损失: A 0.454848 B 0.662787 循环损失: A 0.281115 B 0.258053\n",
      "[迭代925] 判别损失: A 0.435516 B 0.192342 生成损失: A 0.429542 B 0.617901 循环损失: A 0.289858 B 0.328138\n",
      "[迭代930] 判别损失: A 0.436794 B 0.351734 生成损失: A 0.454128 B 0.627865 循环损失: A 0.287234 B 0.387552\n",
      "[迭代935] 判别损失: A 0.370961 B 0.392905 生成损失: A 0.532063 B 0.596391 循环损失: A 0.282173 B 0.398986\n",
      "[迭代940] 判别损失: A 0.616579 B 0.350993 生成损失: A 0.471752 B 0.558314 循环损失: A 0.267446 B 0.340191\n",
      "[迭代945] 判别损失: A 0.560142 B 0.330626 生成损失: A 0.372901 B 0.561627 循环损失: A 0.261877 B 0.307208\n",
      "[迭代950] 判别损失: A 0.353872 B 0.505091 生成损失: A 0.486635 B 0.552793 循环损失: A 0.259394 B 0.368817\n",
      "[迭代955] 判别损失: A 0.494137 B 0.389396 生成损失: A 0.393814 B 0.573722 循环损失: A 0.289338 B 0.298103\n",
      "[迭代960] 判别损失: A 0.559677 B 0.503645 生成损失: A 0.355629 B 0.548328 循环损失: A 0.202902 B 0.249366\n",
      "[迭代965] 判别损失: A 0.458963 B 0.283707 生成损失: A 0.354969 B 0.656959 循环损失: A 0.318512 B 0.244855\n",
      "[迭代970] 判别损失: A 0.545855 B 0.331741 生成损失: A 0.364591 B 0.627868 循环损失: A 0.255133 B 0.270719\n",
      "[迭代975] 判别损失: A 0.436047 B 0.381894 生成损失: A 0.416789 B 0.629621 循环损失: A 0.299852 B 0.279316\n",
      "[迭代980] 判别损失: A 0.389983 B 0.226563 生成损失: A 0.487821 B 0.547797 循环损失: A 0.234518 B 0.185308\n",
      "[迭代985] 判别损失: A 0.351711 B 0.147627 生成损失: A 0.519944 B 0.692823 循环损失: A 0.302401 B 0.287724\n",
      "[迭代990] 判别损失: A 0.453079 B 0.283339 生成损失: A 0.521588 B 0.560335 循环损失: A 0.279538 B 0.254067\n",
      "[迭代995] 判别损失: A 0.466552 B 0.485104 生成损失: A 0.522133 B 0.611689 循环损失: A 0.242513 B 0.306338\n",
      "[迭代1000] 判别损失: A 0.446291 B 0.678709 生成损失: A 0.486405 B 0.499363 循环损失: A 0.291839 B 0.251138\n",
      "[迭代1005] 判别损失: A 0.504393 B 0.371041 生成损失: A 0.457111 B 0.580110 循环损失: A 0.258069 B 0.254082\n",
      "[迭代1010] 判别损失: A 0.653549 B 0.335299 生成损失: A 0.324138 B 0.587716 循环损失: A 0.212812 B 0.239361\n",
      "[迭代1015] 判别损失: A 0.491934 B 0.266444 生成损失: A 0.378934 B 0.699916 循环损失: A 0.264255 B 0.365194\n",
      "[迭代1020] 判别损失: A 0.207801 B 0.088958 生成损失: A 0.509618 B 0.669913 循环损失: A 0.248763 B 0.272153\n",
      "[迭代1025] 判别损失: A 0.170064 B 0.058366 生成损失: A 0.625739 B 0.739967 循环损失: A 0.337072 B 0.431811\n",
      "[迭代1030] 判别损失: A 0.205844 B 0.183394 生成损失: A 0.580378 B 0.675933 循环损失: A 0.248149 B 0.236296\n",
      "[迭代1035] 判别损失: A 0.166883 B 0.277857 生成损失: A 0.638784 B 0.620322 循环损失: A 0.256479 B 0.220859\n",
      "[迭代1040] 判别损失: A 0.269194 B 0.216583 生成损失: A 0.621904 B 0.645838 循环损失: A 0.283369 B 0.282683\n",
      "[迭代1045] 判别损失: A 0.471626 B 0.354516 生成损失: A 0.452888 B 0.571713 循环损失: A 0.268935 B 0.266310\n",
      "[迭代1050] 判别损失: A 0.448708 B 0.393411 生成损失: A 0.402149 B 0.525074 循环损失: A 0.218379 B 0.249013\n",
      "[迭代1055] 判别损失: A 0.444069 B 0.294201 生成损失: A 0.518137 B 0.613676 循环损失: A 0.240202 B 0.278141\n",
      "[迭代1060] 判别损失: A 0.515774 B 0.185379 生成损失: A 0.441501 B 0.689075 循环损失: A 0.258044 B 0.264842\n",
      "[迭代1065] 判别损失: A 0.482896 B 0.312336 生成损失: A 0.406923 B 0.619868 循环损失: A 0.249285 B 0.234393\n",
      "[迭代1070] 判别损失: A 0.329224 B 0.355603 生成损失: A 0.543471 B 0.534456 循环损失: A 0.326911 B 0.314060\n",
      "[迭代1075] 判别损失: A 0.309148 B 0.267285 生成损失: A 0.589086 B 0.540810 循环损失: A 0.274100 B 0.235902\n",
      "[迭代1080] 判别损失: A 0.411059 B 0.495695 生成损失: A 0.440008 B 0.401152 循环损失: A 0.261333 B 0.213390\n",
      "[迭代1085] 判别损失: A 0.399338 B 0.370202 生成损失: A 0.455611 B 0.591065 循环损失: A 0.243498 B 0.247833\n",
      "[迭代1090] 判别损失: A 0.441157 B 0.268110 生成损失: A 0.478878 B 0.648334 循环损失: A 0.295243 B 0.279242\n",
      "[迭代1095] 判别损失: A 0.439139 B 0.390991 生成损失: A 0.428651 B 0.600673 循环损失: A 0.214455 B 0.275940\n",
      "[迭代1100] 判别损失: A 0.429597 B 0.484254 生成损失: A 0.514457 B 0.578850 循环损失: A 0.316224 B 0.205066\n",
      "[迭代1105] 判别损失: A 0.562660 B 0.384401 生成损失: A 0.448826 B 0.598706 循环损失: A 0.347591 B 0.227592\n",
      "[迭代1110] 判别损失: A 0.450733 B 0.264044 生成损失: A 0.488562 B 0.649613 循环损失: A 0.267007 B 0.227666\n",
      "[迭代1115] 判别损失: A 0.351215 B 0.432739 生成损失: A 0.495345 B 0.601018 循环损失: A 0.290437 B 0.205936\n",
      "[迭代1120] 判别损失: A 0.464055 B 0.478886 生成损失: A 0.515545 B 0.581974 循环损失: A 0.251904 B 0.244511\n",
      "[迭代1125] 判别损失: A 0.575831 B 0.307113 生成损失: A 0.385836 B 0.633246 循环损失: A 0.204804 B 0.247951\n",
      "[迭代1130] 判别损失: A 0.589176 B 0.279924 生成损失: A 0.474076 B 0.640368 循环损失: A 0.243686 B 0.235290\n",
      "[迭代1135] 判别损失: A 0.550207 B 0.398629 生成损失: A 0.323190 B 0.587646 循环损失: A 0.280959 B 0.254738\n",
      "[迭代1140] 判别损失: A 0.468542 B 0.365939 生成损失: A 0.393072 B 0.577037 循环损失: A 0.258659 B 0.219233\n",
      "[迭代1145] 判别损失: A 0.455275 B 0.725751 生成损失: A 0.350754 B 0.459996 循环损失: A 0.272187 B 0.244274\n",
      "[迭代1150] 判别损失: A 0.424995 B 0.690344 生成损失: A 0.424508 B 0.432880 循环损失: A 0.281019 B 0.237797\n",
      "[迭代1155] 判别损失: A 0.425001 B 0.534870 生成损失: A 0.470223 B 0.532236 循环损失: A 0.271488 B 0.252215\n",
      "[迭代1160] 判别损失: A 0.443809 B 0.439584 生成损失: A 0.421879 B 0.457445 循环损失: A 0.245615 B 0.231381\n",
      "[迭代1165] 判别损失: A 0.451939 B 0.339335 生成损失: A 0.432436 B 0.493812 循环损失: A 0.265782 B 0.239450\n",
      "[迭代1170] 判别损失: A 0.622094 B 0.289904 生成损失: A 0.387689 B 0.561606 循环损失: A 0.275281 B 0.272442\n",
      "[迭代1175] 判别损失: A 0.444284 B 0.525951 生成损失: A 0.456036 B 0.465592 循环损失: A 0.261716 B 0.207582\n",
      "[迭代1180] 判别损失: A 0.451992 B 0.429687 生成损失: A 0.448407 B 0.475861 循环损失: A 0.287545 B 0.268052\n",
      "[迭代1185] 判别损失: A 0.232429 B 0.496266 生成损失: A 0.591257 B 0.575348 循环损失: A 0.337359 B 0.286108\n",
      "[迭代1190] 判别损失: A 0.438209 B 0.146636 生成损失: A 0.465775 B 0.641022 循环损失: A 0.295667 B 0.360152\n",
      "[迭代1195] 判别损失: A 0.252652 B 0.476076 生成损失: A 0.554069 B 0.485568 循环损失: A 0.297232 B 0.264556\n",
      "[迭代1200] 判别损失: A 0.204218 B 0.217014 生成损失: A 0.586784 B 0.604480 循环损失: A 0.295108 B 0.233808\n",
      "[迭代1205] 判别损失: A 0.399109 B 0.338385 生成损失: A 0.460514 B 0.536682 循环损失: A 0.241561 B 0.252463\n",
      "[迭代1210] 判别损失: A 0.426235 B 0.254919 生成损失: A 0.460525 B 0.648446 循环损失: A 0.324493 B 0.273773\n",
      "[迭代1215] 判别损失: A 0.352803 B 0.329815 生成损失: A 0.528645 B 0.571689 循环损失: A 0.275407 B 0.238661\n",
      "[迭代1220] 判别损失: A 0.339147 B 0.411956 生成损失: A 0.550660 B 0.510499 循环损失: A 0.385566 B 0.312471\n",
      "[迭代1225] 判别损失: A 0.448029 B 0.518554 生成损失: A 0.519227 B 0.484216 循环损失: A 0.212042 B 0.236434\n",
      "[迭代1230] 判别损失: A 0.473839 B 0.458286 生成损失: A 0.496451 B 0.470228 循环损失: A 0.295440 B 0.266051\n",
      "[迭代1235] 判别损失: A 0.353367 B 0.589771 生成损失: A 0.522338 B 0.502896 循环损失: A 0.275075 B 0.269039\n",
      "[迭代1240] 判别损失: A 0.545788 B 0.342856 生成损失: A 0.479831 B 0.531749 循环损失: A 0.225136 B 0.290067\n",
      "[迭代1245] 判别损失: A 0.406292 B 0.447773 生成损失: A 0.521837 B 0.471280 循环损失: A 0.312479 B 0.272495\n",
      "[迭代1250] 判别损失: A 0.544750 B 0.459417 生成损失: A 0.397796 B 0.477836 循环损失: A 0.277806 B 0.279070\n",
      "[迭代1255] 判别损失: A 0.390931 B 0.287627 生成损失: A 0.548034 B 0.513102 循环损失: A 0.281752 B 0.241788\n",
      "[迭代1260] 判别损失: A 0.461179 B 0.259142 生成损失: A 0.479129 B 0.605163 循环损失: A 0.197134 B 0.252251\n",
      "[迭代1265] 判别损失: A 0.527287 B 0.362589 生成损失: A 0.513147 B 0.644500 循环损失: A 0.277832 B 0.388304\n",
      "[迭代1270] 判别损失: A 0.342513 B 0.516985 生成损失: A 0.494826 B 0.526602 循环损失: A 0.267740 B 0.250022\n",
      "[迭代1275] 判别损失: A 0.330902 B 0.591919 生成损失: A 0.524537 B 0.441492 循环损失: A 0.302008 B 0.202590\n",
      "[迭代1280] 判别损失: A 0.313784 B 0.280424 生成损失: A 0.511674 B 0.573107 循环损失: A 0.284193 B 0.343902\n",
      "[迭代1285] 判别损失: A 0.579928 B 0.536753 生成损失: A 0.427809 B 0.524169 循环损失: A 0.242525 B 0.329858\n",
      "[迭代1290] 判别损失: A 0.465191 B 0.458103 生成损失: A 0.477665 B 0.444623 循环损失: A 0.230607 B 0.286620\n",
      "[迭代1295] 判别损失: A 0.395254 B 0.465653 生成损失: A 0.513257 B 0.594069 循环损失: A 0.289792 B 0.271634\n",
      "[迭代1300] 判别损失: A 0.358193 B 0.385153 生成损失: A 0.599823 B 0.538523 循环损失: A 0.239508 B 0.340864\n",
      "[迭代1305] 判别损失: A 0.503961 B 0.371695 生成损失: A 0.412816 B 0.557758 循环损失: A 0.218367 B 0.226320\n",
      "[迭代1310] 判别损失: A 0.493502 B 0.261416 生成损失: A 0.485913 B 0.590574 循环损失: A 0.248846 B 0.254719\n",
      "[迭代1315] 判别损失: A 0.506675 B 0.297634 生成损失: A 0.516486 B 0.558866 循环损失: A 0.270097 B 0.182820\n",
      "[迭代1320] 判别损失: A 0.309668 B 0.352982 生成损失: A 0.514861 B 0.641089 循环损失: A 0.292762 B 0.256577\n",
      "[迭代1325] 判别损失: A 0.426403 B 0.429793 生成损失: A 0.436435 B 0.505424 循环损失: A 0.246279 B 0.193029\n",
      "[迭代1330] 判别损失: A 0.546413 B 0.418514 生成损失: A 0.474360 B 0.543215 循环损失: A 0.228507 B 0.232280\n",
      "[迭代1335] 判别损失: A 0.391349 B 0.346695 生成损失: A 0.480195 B 0.514290 循环损失: A 0.218701 B 0.264987\n",
      "[迭代1340] 判别损失: A 0.452560 B 0.344700 生成损失: A 0.487804 B 0.577949 循环损失: A 0.245696 B 0.246507\n",
      "[迭代1345] 判别损失: A 0.522280 B 0.555978 生成损失: A 0.421177 B 0.563695 循环损失: A 0.227715 B 0.284754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[迭代1350] 判别损失: A 0.452833 B 0.629250 生成损失: A 0.430399 B 0.631518 循环损失: A 0.269014 B 0.307281\n",
      "[迭代1355] 判别损失: A 0.324069 B 0.405570 生成损失: A 0.488937 B 0.443881 循环损失: A 0.242581 B 0.208794\n",
      "[迭代1360] 判别损失: A 0.300645 B 0.414689 生成损失: A 0.544161 B 0.562328 循环损失: A 0.283893 B 0.372998\n",
      "[迭代1365] 判别损失: A 0.407380 B 0.464191 生成损失: A 0.502841 B 0.406470 循环损失: A 0.215391 B 0.198265\n",
      "[迭代1370] 判别损失: A 0.366484 B 0.381721 生成损失: A 0.558316 B 0.552927 循环损失: A 0.221027 B 0.227819\n",
      "[迭代1375] 判别损失: A 0.561285 B 0.391081 生成损失: A 0.413733 B 0.543019 循环损失: A 0.230342 B 0.301985\n",
      "[迭代1380] 判别损失: A 0.424406 B 0.651354 生成损失: A 0.461305 B 0.431021 循环损失: A 0.250404 B 0.235762\n",
      "[迭代1385] 判别损失: A 0.325569 B 0.473094 生成损失: A 0.554514 B 0.485018 循环损失: A 0.256609 B 0.234950\n",
      "[迭代1390] 判别损失: A 0.494698 B 0.369719 生成损失: A 0.446662 B 0.549208 循环损失: A 0.268958 B 0.301772\n",
      "[迭代1395] 判别损失: A 0.457142 B 0.310630 生成损失: A 0.477145 B 0.502735 循环损失: A 0.252367 B 0.226182\n",
      "[迭代1400] 判别损失: A 0.336601 B 0.291210 生成损失: A 0.573307 B 0.566308 循环损失: A 0.253030 B 0.290430\n",
      "[迭代1405] 判别损失: A 0.443392 B 0.342551 生成损失: A 0.524137 B 0.621455 循环损失: A 0.223852 B 0.280765\n",
      "[迭代1410] 判别损失: A 0.231205 B 0.373075 生成损失: A 0.572742 B 0.499361 循环损失: A 0.252982 B 0.269017\n",
      "[迭代1415] 判别损失: A 0.253266 B 0.493765 生成损失: A 0.597351 B 0.529192 循环损失: A 0.277211 B 0.219436\n",
      "[迭代1420] 判别损失: A 0.276022 B 0.365419 生成损失: A 0.581206 B 0.516713 循环损失: A 0.256001 B 0.201758\n",
      "[迭代1425] 判别损失: A 0.281706 B 0.478406 生成损失: A 0.610187 B 0.441485 循环损失: A 0.215402 B 0.258056\n",
      "[迭代1430] 判别损失: A 0.175299 B 0.401267 生成损失: A 0.676604 B 0.521978 循环损失: A 0.300736 B 0.270798\n",
      "[迭代1435] 判别损失: A 0.359543 B 0.473763 生成损失: A 0.547838 B 0.454990 循环损失: A 0.266334 B 0.237235\n",
      "[迭代1440] 判别损失: A 0.303461 B 0.391700 生成损失: A 0.602850 B 0.498211 循环损失: A 0.228285 B 0.286873\n",
      "[迭代1445] 判别损失: A 0.417743 B 0.299403 生成损失: A 0.452815 B 0.576263 循环损失: A 0.279314 B 0.283179\n",
      "[迭代1450] 判别损失: A 0.249299 B 0.410109 生成损失: A 0.672503 B 0.504897 循环损失: A 0.274296 B 0.267830\n",
      "[迭代1455] 判别损失: A 0.102057 B 0.321117 生成损失: A 0.724514 B 0.605277 循环损失: A 0.375818 B 0.328367\n",
      "[迭代1460] 判别损失: A 0.141316 B 0.437221 生成损失: A 0.763947 B 0.504954 循环损失: A 0.310287 B 0.278711\n",
      "[迭代1465] 判别损失: A 0.281885 B 0.433846 生成损失: A 0.650571 B 0.542474 循环损失: A 0.265147 B 0.229817\n",
      "[迭代1470] 判别损失: A 0.382256 B 0.369583 生成损失: A 0.636912 B 0.552540 循环损失: A 0.238287 B 0.260459\n",
      "[迭代1475] 判别损失: A 0.514551 B 0.229938 生成损失: A 0.633664 B 0.608287 循环损失: A 0.255104 B 0.186012\n",
      "[迭代1480] 判别损失: A 0.297136 B 0.320235 生成损失: A 0.630495 B 0.653484 循环损失: A 0.250978 B 0.247362\n",
      "[迭代1485] 判别损失: A 0.370602 B 0.368676 生成损失: A 0.602158 B 0.504527 循环损失: A 0.219771 B 0.261515\n",
      "[迭代1490] 判别损失: A 0.367193 B 0.356007 生成损失: A 0.581977 B 0.559172 循环损失: A 0.258867 B 0.256278\n",
      "[迭代1495] 判别损失: A 0.387149 B 0.389686 生成损失: A 0.665390 B 0.491447 循环损失: A 0.296502 B 0.260681\n",
      "[迭代1500] 判别损失: A 0.229161 B 0.401239 生成损失: A 0.729091 B 0.563190 循环损失: A 0.229689 B 0.364507\n",
      "[迭代1505] 判别损失: A 0.240206 B 0.478938 生成损失: A 0.658743 B 0.422658 循环损失: A 0.272615 B 0.242401\n",
      "[迭代1510] 判别损失: A 0.305423 B 0.302090 生成损失: A 0.598825 B 0.496556 循环损失: A 0.267756 B 0.304939\n",
      "[迭代1515] 判别损失: A 0.279003 B 0.394237 生成损失: A 0.565079 B 0.522249 循环损失: A 0.268489 B 0.255080\n",
      "[迭代1520] 判别损失: A 0.633667 B 0.516962 生成损失: A 0.593002 B 0.513273 循环损失: A 0.229839 B 0.291901\n",
      "[迭代1525] 判别损失: A 0.374796 B 0.406829 生成损失: A 0.491815 B 0.466604 循环损失: A 0.285862 B 0.256062\n",
      "[迭代1530] 判别损失: A 0.447066 B 0.109562 生成损失: A 0.540472 B 0.710801 循环损失: A 0.222651 B 0.475491\n",
      "[迭代1535] 判别损失: A 0.343402 B 0.519892 生成损失: A 0.578744 B 0.614213 循环损失: A 0.311092 B 0.247352\n",
      "[迭代1540] 判别损失: A 0.301255 B 0.421639 生成损失: A 0.601960 B 0.426069 循环损失: A 0.265236 B 0.211058\n",
      "[迭代1545] 判别损失: A 0.355379 B 0.295185 生成损失: A 0.603041 B 0.540608 循环损失: A 0.274806 B 0.237603\n",
      "[迭代1550] 判别损失: A 0.238918 B 0.385614 生成损失: A 0.602508 B 0.538509 循环损失: A 0.223918 B 0.231063\n",
      "[迭代1555] 判别损失: A 0.315928 B 0.437168 生成损失: A 0.630894 B 0.488278 循环损失: A 0.238066 B 0.250225\n",
      "[迭代1560] 判别损失: A 0.247240 B 0.355263 生成损失: A 0.696850 B 0.566923 循环损失: A 0.393944 B 0.228222\n",
      "[迭代1565] 判别损失: A 0.400736 B 0.488495 生成损失: A 0.582883 B 0.451973 循环损失: A 0.258883 B 0.185955\n",
      "[迭代1570] 判别损失: A 0.389054 B 0.287144 生成损失: A 0.583731 B 0.587918 循环损失: A 0.256435 B 0.268790\n",
      "[迭代1575] 判别损失: A 0.412483 B 0.422490 生成损失: A 0.552294 B 0.536117 循环损失: A 0.270915 B 0.198310\n",
      "[迭代1580] 判别损失: A 0.264523 B 0.276707 生成损失: A 0.609372 B 0.530923 循环损失: A 0.255759 B 0.192568\n",
      "[迭代1585] 判别损失: A 0.204478 B 0.232907 生成损失: A 0.630761 B 0.623734 循环损失: A 0.260970 B 0.281048\n",
      "[迭代1590] 判别损失: A 0.444287 B 0.649290 生成损失: A 0.532441 B 0.424606 循环损失: A 0.218307 B 0.167858\n",
      "[迭代1595] 判别损失: A 0.430796 B 0.463144 生成损失: A 0.556036 B 0.493796 循环损失: A 0.272222 B 0.218022\n",
      "[迭代1600] 判别损失: A 0.304562 B 0.239986 生成损失: A 0.617864 B 0.566523 循环损失: A 0.288070 B 0.275310\n",
      "[迭代1605] 判别损失: A 0.256214 B 0.386314 生成损失: A 0.677278 B 0.561559 循环损失: A 0.275576 B 0.239831\n",
      "[迭代1610] 判别损失: A 0.166152 B 0.341151 生成损失: A 0.701011 B 0.555417 循环损失: A 0.205777 B 0.196596\n",
      "[迭代1615] 判别损失: A 0.372434 B 0.435363 生成损失: A 0.609841 B 0.486097 循环损失: A 0.267411 B 0.235784\n",
      "[迭代1620] 判别损失: A 0.195910 B 0.322564 生成损失: A 0.675249 B 0.523275 循环损失: A 0.230813 B 0.184462\n",
      "[迭代1625] 判别损失: A 0.605024 B 0.354343 生成损失: A 0.461785 B 0.520638 循环损失: A 0.226052 B 0.226373\n",
      "[迭代1630] 判别损失: A 0.346062 B 0.298150 生成损失: A 0.603005 B 0.649735 循环损失: A 0.272523 B 0.286285\n",
      "[迭代1635] 判别损失: A 0.296949 B 0.400633 生成损失: A 0.591502 B 0.500428 循环损失: A 0.242317 B 0.245830\n",
      "[迭代1640] 判别损失: A 0.441160 B 0.524890 生成损失: A 0.520653 B 0.469579 循环损失: A 0.221423 B 0.189600\n",
      "[迭代1645] 判别损失: A 0.503460 B 0.401524 生成损失: A 0.432395 B 0.521183 循环损失: A 0.216514 B 0.215958\n",
      "[迭代1650] 判别损失: A 0.331727 B 0.262309 生成损失: A 0.652857 B 0.588000 循环损失: A 0.270844 B 0.263205\n",
      "[迭代1655] 判别损失: A 0.417672 B 0.494012 生成损失: A 0.523060 B 0.450822 循环损失: A 0.190261 B 0.224475\n",
      "[迭代1660] 判别损失: A 0.455763 B 0.460839 生成损失: A 0.468653 B 0.645388 循环损失: A 0.253397 B 0.226300\n",
      "[迭代1665] 判别损失: A 0.358997 B 0.413872 生成损失: A 0.539058 B 0.466350 循环损失: A 0.235040 B 0.244048\n",
      "[迭代1670] 判别损失: A 0.496543 B 0.508624 生成损失: A 0.502794 B 0.462763 循环损失: A 0.266584 B 0.204795\n",
      "[迭代1675] 判别损失: A 0.317254 B 0.307978 生成损失: A 0.622366 B 0.590143 循环损失: A 0.297679 B 0.228816\n",
      "[迭代1680] 判别损失: A 0.303738 B 0.366488 生成损失: A 0.562909 B 0.610418 循环损失: A 0.196108 B 0.193940\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-52fa2e83af4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mEPOCH_NUM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0merr\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0merr_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-17d0227e61b3>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(batchsize)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"\"\"\n\u001b[1;32m      8\u001b[0m     \u001b[0mepa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mepb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_B\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-af79ccd488f2>\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(self, batchsize)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-af79ccd488f2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-af79ccd488f2>\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(fn, image_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mimage_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0m图像大小\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \"\"\"\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#切割图像(截取图像中间的最大正方形，然后将大小调整至输入大小)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2587\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2589\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2591\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#训练代码\n",
    "import time\n",
    "start_t = time.time()\n",
    "\n",
    "EPOCH_NUM = 100\n",
    "epoch = 0\n",
    "\n",
    "DISPLAY_INTERVAL = 5\n",
    "SNAPSHOT_INTERVAL = 50\n",
    "SAVE_INTERVAL = 200\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "iter_cnt = 0\n",
    "err_sum = np.zeros(6)\n",
    "\n",
    "while epoch < EPOCH_NUM:       \n",
    "    epoch, A, B = train_batch(BATCH_SIZE) \n",
    "    err  = model.train(A, B)\n",
    "    err_sum += np.array(err)\n",
    "\n",
    "    iter_cnt += 1\n",
    "\n",
    "    if (iter_cnt % DISPLAY_INTERVAL == 0):\n",
    "        err_avg = err_sum / DISPLAY_INTERVAL\n",
    "        print('[迭代%d] 判别损失: A %f B %f 生成损失: A %f B %f 循环损失: A %f B %f'\n",
    "        % (iter_cnt, \n",
    "        err_avg[0], err_avg[1], err_avg[2], err_avg[3], err_avg[4], err_avg[5]),\n",
    "        )      \n",
    "        err_sum = np.zeros_like(err_sum)\n",
    "\n",
    "\n",
    "    if (iter_cnt % SNAPSHOT_INTERVAL == 0):\n",
    "        A = train_A.get_pics(4)\n",
    "        B = train_B.get_pics(4)\n",
    "        snapshot(model.cycleA, model.cycleB, A, B).save(\"./snapshot/{}.png\".format(iter_cnt))\n",
    "\n",
    "    if (iter_cnt % SAVE_INTERVAL == 0):\n",
    "        model.save(path = \"./models/model-{}\".format(iter_cnt))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
